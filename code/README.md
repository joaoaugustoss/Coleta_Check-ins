# Analysis of Urban Mobility through Location-Based Social Networks: A Case Study in Smart Cities.

The number of people using social media is growing every day, and as a result, the amount of data available about users is also increasing. Currently, about $4,8$ billion people use social media worldwide, which is approximately $59$\% of the global population. Given this scenario, the literature contains works that address techniques for data collection and sampling from social media, allowing for the interpretation of such data for analysis in different domains, such as urban mobility, region classification, among others. This study utilizes Location-Based Social Networks (LBSNs) to analyze human mobility within urban centers. Check-ins collected from around the world were used to identify distinct behavior patterns among inhabitants from different regions of the planet, based on the concentration of records in specific locations. The results indicate that, despite the disparity in the spatial distribution of the data, LBSNs are capable of depicting the reality of cities, even across different cultures, making them a valuable means for collecting data for smart cities due to data availability and the easy scalability of their applications.

## Code
### twitter_api.py
The file [twitter_api.py](twitter_api.py) contains the code developed for collecting check-ins posted on Foursquare Swarm and published on Twitter, which are used to build the database for further use in the project. To make the code work, requests were made to the [Twitter API](https://developer.twitter.com/en) using the [Filtered Stream](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/introduction) service, which returns all the tweets posted during the code execution with a specific tag chosen by the developer.

### filter.py
The file [filter.py](filter.py) is the code developed for processing the data collected by the previous code. The processing is done by opening the file generated by the API collection. The first step in the code is to open the file generated by the API to store the necessary attributes and discard what was considered irrelevant. To achieve this, the methodology of parsing the HTML code of the website was used using the Python language, observing the required patterns for extracting the necessary attributes. In the first version, these attributes were used as parameters in the [Foursquare Places API](https://developer.foursquare.com/docs/places-api-overview), but due to the low limit of available requests, we started retrieving all the necessary attributes directly from the HTML code extracted from Twitter's link. The following attributes are used for building the CSV file:

<ul>
  <li>Foursquare venue identifier</li>  
  <li>Swarm user identifier</li>  
  <li>Name of the venue where the check-in was made</li>  
  <li>Collected link from Twitter</li>  
  <li>Category of the venue where the check-in was made</li>  
  <li>Country of the venue where the check-in was made</li>  
  <li>City of the venue where the check-in was made</li>
  <li>Time when the check-in was shared on Twitter
  <li>Latitude of the venue where the check-in was made</li> 
  <li>Longitude of the venue where the check-in was made</li>
</ul>

### update.py
The file [update.py](update.py) is the code developed to handle different versions of the database, allowing it to be updated according to the chosen attributes for its composition.
